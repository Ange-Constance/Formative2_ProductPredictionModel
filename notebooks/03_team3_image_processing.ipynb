{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3e6abc",
   "metadata": {},
   "source": [
    "# Team 3: Image Processing & Facial Recognition Model\n",
    "\n",
    "This notebook demonstrates the complete pipeline for facial recognition, including:\n",
    "1. Image collection and loading\n",
    "2. Image augmentation and preprocessing\n",
    "3. Feature extraction\n",
    "4. Model training and evaluation\n",
    "5. Prediction and authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e05a7c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391124eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ad44c",
   "metadata": {},
   "source": [
    "## 2. Image Collection & Display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dce221",
   "metadata": {},
   "source": [
    "### 2.1 Load Member Images\n",
    "\n",
    "First, ensure your images are organized in the following structure:\n",
    "```\n",
    "data/raw/images/\n",
    "├── member1/\n",
    "│   ├── neutral.jpg\n",
    "│   ├── smiling.jpg\n",
    "│   └── surprised.jpg\n",
    "├── member2/\n",
    "├── member3/\n",
    "└── member4/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166232c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Load an image from file.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_member_images(member_id, raw_images_path):\n",
    "    \"\"\"Load all images for a specific member.\"\"\"\n",
    "    member_path = os.path.join(raw_images_path, member_id)\n",
    "    \n",
    "    if not os.path.exists(member_path):\n",
    "        raise FileNotFoundError(f\"Member directory not found: {member_path}\")\n",
    "    \n",
    "    images = {}\n",
    "    for filename in os.listdir(member_path):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(member_path, filename)\n",
    "            images[filename] = load_image(image_path)\n",
    "    \n",
    "    return images\n",
    "\n",
    "\n",
    "def display_images(images_dict, title=\"Facial Images\"):\n",
    "    \"\"\"Display multiple images in a grid.\"\"\"\n",
    "    num_images = len(images_dict)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (name, image) in enumerate(images_dict.items()):\n",
    "        # Convert BGR to RGB for display\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        axes[idx].imshow(image_rgb)\n",
    "        axes[idx].set_title(name)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define paths\n",
    "raw_images_path = \"../data/raw/images\"\n",
    "processed_path = \"../data/processed\"\n",
    "models_path = \"../models\"\n",
    "\n",
    "print(f\"Raw images path: {raw_images_path}\")\n",
    "print(f\"Processed data path: {processed_path}\")\n",
    "print(f\"Models path: {models_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34244e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load and display Alliance images\n",
    "try:\n",
    "    alliance_images = load_member_images(\"Alliance\", raw_images_path)\n",
    "    print(f\"Successfully loaded {len(alliance_images)} images for Alliance\")\n",
    "    print(f\"Images: {list(alliance_images.keys())}\")\n",
    "    display_images(alliance_images, \"Alliance - Facial Images\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Please ensure images are placed in data/raw/images/<member>/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbe550",
   "metadata": {},
   "source": [
    "## 3. Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image, angle=15):\n",
    "    \"\"\"Rotate image by specified angle.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, matrix, (w, h))\n",
    "    return rotated\n",
    "\n",
    "\n",
    "def flip_image(image, axis=1):\n",
    "    \"\"\"Flip image horizontally or vertically.\"\"\"\n",
    "    return cv2.flip(image, axis)\n",
    "\n",
    "\n",
    "def convert_to_grayscale(image):\n",
    "    \"\"\"Convert image to grayscale.\"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "def adjust_brightness(image, brightness_factor=0.7):\n",
    "    \"\"\"Adjust image brightness.\"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
    "    hsv[:, :, 2] = hsv[:, :, 2] * brightness_factor\n",
    "    hsv[:, :, 2] = np.clip(hsv[:, :, 2], 0, 255)\n",
    "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "def apply_augmentations(image, augmentation_list):\n",
    "    \"\"\"Apply multiple augmentations to an image.\"\"\"\n",
    "    augmented_images = [image]  # Include original\n",
    "    \n",
    "    for aug_func in augmentation_list:\n",
    "        augmented_images.append(aug_func(image))\n",
    "    \n",
    "    return augmented_images\n",
    "\n",
    "\n",
    "print(\"Augmentation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65933cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Apply augmentations to Alliance's first image\n",
    "try:\n",
    "    sample_image = list(alliance_images.values())[0]\n",
    "    \n",
    "    # Define augmentations\n",
    "    augmentations = [\n",
    "        ('Original', lambda img: img),\n",
    "        ('Rotated +15°', lambda img: rotate_image(img, 15)),\n",
    "        ('Rotated -15°', lambda img: rotate_image(img, -15)),\n",
    "        ('Flipped', lambda img: flip_image(img)),\n",
    "        ('Grayscale', lambda img: convert_to_grayscale(img)),\n",
    "        ('Brightened', lambda img: adjust_brightness(img, 1.3))\n",
    "    ]\n",
    "    \n",
    "    # Apply augmentations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (aug_name, aug_func) in enumerate(augmentations):\n",
    "        aug_image = aug_func(sample_image)\n",
    "        \n",
    "        if len(aug_image.shape) == 3:\n",
    "            aug_image_rgb = cv2.cvtColor(aug_image, cv2.COLOR_BGR2RGB)\n",
    "            axes[idx].imshow(aug_image_rgb)\n",
    "        else:\n",
    "            axes[idx].imshow(aug_image, cmap='gray')\n",
    "        \n",
    "        axes[idx].set_title(aug_name)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Image Augmentations', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Augmentation functions are ready for use once images are loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9b15e",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0287e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_histogram_features(image, bins=32):\n",
    "    \"\"\"Extract color histogram features from image.\"\"\"\n",
    "    features = []\n",
    "    for i in range(3):  # For each channel (B, G, R)\n",
    "        hist = cv2.calcHist([image], [i], None, [bins], [0, 256])\n",
    "        features.extend(hist.flatten())\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def extract_edge_features(image):\n",
    "    \"\"\"Extract edge features using Canny edge detection.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    return edges.flatten()\n",
    "\n",
    "\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    \"\"\"\n",
    "    Extract HOG (Histogram of Oriented Gradients) features.\n",
    "    More robust to lighting changes than basic edges.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hog = cv2.HOGDescriptor(\n",
    "        (64, 64),\n",
    "        (16, 16),\n",
    "        (8, 8),\n",
    "        (8, 8),\n",
    "        orientations\n",
    "    )\n",
    "    features = hog.compute(cv2.resize(gray, (64, 64)))\n",
    "    return features.flatten()\n",
    "\n",
    "\n",
    "def extract_color_moments(image):\n",
    "    \"\"\"Extract color moment features (mean, std, skewness per channel).\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for i in range(3):  # For each channel\n",
    "        channel = image[:, :, i].astype(np.float32)\n",
    "        mean = np.mean(channel)\n",
    "        std = np.std(channel)\n",
    "        skewness = np.mean((channel - mean) ** 3) / (std ** 3 + 1e-6)\n",
    "        \n",
    "        features.extend([mean, std, skewness])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def extract_lbp_features(image, num_points=8, radius=1):\n",
    "    \"\"\"\n",
    "    Extract Local Binary Pattern (LBP) features.\n",
    "    More invariant to lighting changes than histograms.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # LBP computation\n",
    "    def lbp_computation(img, p=num_points, r=radius):\n",
    "        rows, cols = img.shape\n",
    "        lbp = np.zeros((rows, cols), dtype=np.uint8)\n",
    "        \n",
    "        for i in range(r, rows - r):\n",
    "            for j in range(r, cols - r):\n",
    "                center = img[i, j]\n",
    "                lbp_val = 0\n",
    "                \n",
    "                for k in range(p):\n",
    "                    angle = 2 * np.pi * k / p\n",
    "                    x = int(r * np.cos(angle))\n",
    "                    y = int(r * np.sin(angle))\n",
    "                    neighbor = img[i + y, j + x]\n",
    "                    lbp_val = (lbp_val << 1) | (1 if neighbor >= center else 0)\n",
    "                \n",
    "                lbp[i, j] = lbp_val\n",
    "        \n",
    "        return lbp\n",
    "    \n",
    "    lbp = lbp_computation(gray, num_points, radius)\n",
    "    hist = cv2.calcHist([lbp], [0], None, [256], [0, 256])\n",
    "    \n",
    "    return hist.flatten()\n",
    "\n",
    "\n",
    "def extract_all_features(image, feature_types=None):\n",
    "    \"\"\"\n",
    "    Extract all specified features from an image.\n",
    "    \n",
    "    Feature types:\n",
    "    - histogram: Color histogram (96 features)\n",
    "    - edges: Canny edges (50176 features)\n",
    "    - hog: Histogram of Oriented Gradients - MORE ROBUST (1764 features)\n",
    "    - color_moments: Mean, std, skewness per channel (9 features)\n",
    "    - lbp: Local Binary Patterns - MORE INVARIANT (256 features)\n",
    "    \"\"\"\n",
    "    if feature_types is None:\n",
    "        # Default: Use robust features\n",
    "        feature_types = ['histogram', 'hog', 'color_moments', 'lbp']\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    if 'histogram' in feature_types:\n",
    "        features.extend(extract_histogram_features(image))\n",
    "    \n",
    "    if 'edges' in feature_types:\n",
    "        features.extend(extract_edge_features(image))\n",
    "    \n",
    "    if 'hog' in feature_types:\n",
    "        features.extend(extract_hog_features(image))\n",
    "    \n",
    "    if 'color_moments' in feature_types:\n",
    "        features.extend(extract_color_moments(image))\n",
    "    \n",
    "    if 'lbp' in feature_types:\n",
    "        features.extend(extract_lbp_features(image))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "print(\"Advanced feature extraction functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937670c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract features from sample image\n",
    "try:\n",
    "    sample_image = list(alliance_images.values())[0]\n",
    "    \n",
    "    # Extract features using advanced methods\n",
    "    features = extract_all_features(sample_image)\n",
    "    \n",
    "    print(f\"Sample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Total Features Extracted: {len(features)}\")\n",
    "    print(f\"Feature Vector Shape: {features.shape}\")\n",
    "    print(f\"\\nFeature Statistics:\")\n",
    "    print(f\"  Min: {features.min():.4f}\")\n",
    "    print(f\"  Max: {features.max():.4f}\")\n",
    "    print(f\"  Mean: {features.mean():.4f}\")\n",
    "    print(f\"  Std: {features.std():.4f}\")\n",
    "    print(f\"\\nFeatures included:\")\n",
    "    print(f\"  ✓ Histogram (Color distribution)\")\n",
    "    print(f\"  ✓ HOG (Histogram of Oriented Gradients - lighting invariant)\")\n",
    "    print(f\"  ✓ Color Moments (Mean, std, skewness per channel)\")\n",
    "    print(f\"  ✓ LBP (Local Binary Patterns - invariant to monotonic brightness)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Feature extraction functions are ready for use once images are loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99924902",
   "metadata": {},
   "source": [
    "## 5. Data Preparation & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793476e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(raw_images_path, members=['Alliance', 'Ange', 'Elissa', 'Terry']):\n",
    "    \"\"\"Prepare training data from raw images using advanced features.\"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    label_encoder = {}\n",
    "    reverse_label_encoder = {}\n",
    "    \n",
    "    # Create label encoder\n",
    "    for idx, member in enumerate(members):\n",
    "        label_encoder[member] = idx\n",
    "        reverse_label_encoder[idx] = member\n",
    "    \n",
    "    print(\"Loading and processing images...\")\n",
    "    print(\"Feature types: Histogram + HOG + Color Moments + LBP\")\n",
    "    \n",
    "    for member_id in members:\n",
    "        print(f\"Processing {member_id}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load member's images\n",
    "            member_images = load_member_images(member_id, raw_images_path)\n",
    "            \n",
    "            for img_name, image in member_images.items():\n",
    "                # Resize image to standard size for consistency\n",
    "                image_resized = cv2.resize(image, (224, 224))\n",
    "                \n",
    "                # Extract advanced features from original image\n",
    "                feature_vector = extract_all_features(image_resized)\n",
    "                if feature_vector is not None and len(feature_vector) > 0:\n",
    "                    X_list.append(feature_vector)\n",
    "                    y_list.append(label_encoder[member_id])\n",
    "                \n",
    "                # Apply augmentations and extract features\n",
    "                augmentations = [\n",
    "                    lambda img: rotate_image(img, 15),\n",
    "                    lambda img: flip_image(img),\n",
    "                    lambda img: convert_to_grayscale(img)\n",
    "                ]\n",
    "                \n",
    "                augmented_images = apply_augmentations(image_resized, augmentations)\n",
    "                \n",
    "                for aug_img in augmented_images[1:]:  # Skip original\n",
    "                    # Ensure image is RGB\n",
    "                    if len(aug_img.shape) == 2:  # Grayscale\n",
    "                        aug_img = cv2.cvtColor(aug_img, cv2.COLOR_GRAY2BGR)\n",
    "                    elif aug_img.shape[2] == 4:  # RGBA\n",
    "                        aug_img = cv2.cvtColor(aug_img, cv2.COLOR_RGBA2BGR)\n",
    "                    \n",
    "                    # Ensure consistent size\n",
    "                    aug_img = cv2.resize(aug_img, (224, 224))\n",
    "                    \n",
    "                    feature_vector = extract_all_features(aug_img)\n",
    "                    if feature_vector is not None and len(feature_vector) > 0:\n",
    "                        X_list.append(feature_vector)\n",
    "                        y_list.append(label_encoder[member_id])\n",
    "        \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Warning: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy array with consistent shape\n",
    "    if len(X_list) > 0:\n",
    "        # Ensure all feature vectors have the same length\n",
    "        feature_length = len(X_list[0])\n",
    "        X_list = [feat for feat in X_list if len(feat) == feature_length]\n",
    "        X = np.array(X_list)\n",
    "    else:\n",
    "        X = np.array([])\n",
    "    \n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f\"\\nTraining data shape: {X.shape}\")\n",
    "    print(f\"Number of samples: {len(y)}\")\n",
    "    if len(X_list) > 0:\n",
    "        print(f\"Feature vector length: {len(X_list[0])}\")\n",
    "        print(f\"Features per image: {len(X_list[0])} (Advanced: HOG + LBP + Histogram + Color Moments)\")\n",
    "    \n",
    "    return X, y, label_encoder, reverse_label_encoder\n",
    "\n",
    "\n",
    "print(\"Advanced data preparation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c971e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "try:\n",
    "    X, y, label_encoder, reverse_label_encoder = prepare_training_data(raw_images_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING DATA SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Feature dimensions: {X.shape[1]}\")\n",
    "    print(f\"Number of classes: {len(label_encoder)}\")\n",
    "    print(\"\\nClass distribution:\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for label, count in zip(unique, counts):\n",
    "        member = reverse_label_encoder[label]\n",
    "        print(f\"  {member}: {count} samples\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Data preparation functions are ready for use once images are properly organized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with advanced features\n",
    "try:\n",
    "    print(\"Training Facial Recognition Model (with Advanced Features)...\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Feature extraction: HOG + LBP + Histogram + Color Moments\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
    "    print(f\"Test set size: {len(X_test)} samples\")\n",
    "    print(f\"Feature dimension: {X.shape[1]}\")\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining Random Forest Classifier...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"✓ Model training completed!\")\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_train = f1_score(y_train, y_pred_train, average='weighted')\n",
    "    f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL PERFORMANCE (With Advanced Features)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Training Accuracy: {accuracy_train:.4f} ({accuracy_train*100:.2f}%)\")\n",
    "    print(f\"Testing Accuracy:  {accuracy_test:.4f} ({accuracy_test*100:.2f}%)\")\n",
    "    print(f\"Training F1-Score: {f1_train:.4f}\")\n",
    "    print(f\"Testing F1-Score:  {f1_test:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNote: These metrics use robust features (HOG + LBP)\")\n",
    "    print(\"Expected to handle lighting/angle/environment variations better\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Model training will proceed once training data is prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df040649",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(reverse_label_encoder.values()),\n",
    "                yticklabels=list(reverse_label_encoder.values()))\n",
    "    plt.title('Confusion Matrix - Facial Recognition Model')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test,\n",
    "                              target_names=list(reverse_label_encoder.values())))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Evaluation will be performed after model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22ba79",
   "metadata": {},
   "source": [
    "## 7. Save Features and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(processed_path, exist_ok=True)\n",
    "    os.makedirs(models_path, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(models_path, 'facial_recognition_model.pkl')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'label_encoder': label_encoder,\n",
    "            'reverse_label_encoder': reverse_label_encoder\n",
    "        }, f)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save features to CSV\n",
    "    features_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "    features_df['label'] = y\n",
    "    features_df['member'] = features_df['label'].map(reverse_label_encoder)\n",
    "    \n",
    "    csv_path = os.path.join(processed_path, 'image_features.csv')\n",
    "    features_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Features saved to: {csv_path}\")\n",
    "    print(f\"Shape: {features_df.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Files will be saved once model training is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bd420",
   "metadata": {},
   "source": [
    "## 8. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_face(image_path, model, reverse_label_encoder, target_size=(224, 224)):\n",
    "    \"\"\"Predict the user from a facial image.\"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = load_image(image_path)\n",
    "        \n",
    "        # Resize to standard size\n",
    "        image_resized = cv2.resize(image, target_size)\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_all_features(image_resized).reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(features)[0]\n",
    "        probabilities = model.predict_proba(features)[0]\n",
    "        confidence = np.max(probabilities)\n",
    "        \n",
    "        member = reverse_label_encoder[prediction]\n",
    "        \n",
    "        return {\n",
    "            'member': member,\n",
    "            'confidence': confidence,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Prediction function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c003b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction on a sample image\n",
    "try:\n",
    "    test_image_path = os.path.join(raw_images_path, 'Alliance', list(alliance_images.keys())[0])\n",
    "    \n",
    "    result = predict_face(test_image_path, model, reverse_label_encoder)\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\nPrediction Result:\")\n",
    "        print(f\"  Predicted Member: {result['member']}\")\n",
    "        print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "        print(f\"\\n  Probabilities for each member:\")\n",
    "        for member, prob in zip(reverse_label_encoder.values(), result['probabilities']):\n",
    "            print(f\"    {member}: {prob:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Predictions will be available after model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Authentication & Unauthorized Access Detection\n",
    "\n",
    "### 9.1 Define Confidence Threshold and Authentication Logic\n",
    "\n",
    "# Set confidence threshold for authorization\n",
    "CONFIDENCE_THRESHOLD = 0.70  # Minimum 70% confidence to grant access\n",
    "\n",
    "def authenticate_user(image_path, model, reverse_label_encoder, confidence_threshold=CONFIDENCE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Authenticate a user based on facial recognition with confidence threshold.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the facial image\n",
    "        model: Trained facial recognition model\n",
    "        reverse_label_encoder: Mapping from label to member name\n",
    "        confidence_threshold: Minimum confidence required for authorization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with authentication result\n",
    "    \"\"\"\n",
    "    result = predict_face(image_path, model, reverse_label_encoder)\n",
    "    \n",
    "    if result is None:\n",
    "        return {\n",
    "            'authorized': False,\n",
    "            'reason': 'Error: Could not process image',\n",
    "            'member': 'Unknown',\n",
    "            'confidence': 0.0\n",
    "        }\n",
    "    \n",
    "    # Check if confidence meets threshold\n",
    "    if result['confidence'] >= confidence_threshold:\n",
    "        return {\n",
    "            'authorized': True,\n",
    "            'reason': f\" AUTHORIZED - {result['member']} recognized with {result['confidence']*100:.2f}% confidence\",\n",
    "            'member': result['member'],\n",
    "            'confidence': result['confidence']\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'authorized': False,\n",
    "            'reason': f\" DENIED - Confidence too low ({result['confidence']*100:.2f}% < {confidence_threshold*100:.0f}% threshold)\",\n",
    "            'member': result['member'],\n",
    "            'confidence': result['confidence']\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Authentication logic defined.\")\n",
    "print(f\"Confidence Threshold: {CONFIDENCE_THRESHOLD*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e5ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.2 Comprehensive Authentication Tests\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FACIAL RECOGNITION AUTHENTICATION SYSTEM - COMPREHENSIVE TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TEST 1: Authorized User\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 1: AUTHORIZED USER - Known Member (Training Distribution)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Use an authorized member's image from training distribution\n",
    "    authorized_image_path = os.path.join(raw_images_path, 'Alliance', list(alliance_images.keys())[0])\n",
    "    \n",
    "    auth_result = authenticate_user(authorized_image_path, model, reverse_label_encoder)\n",
    "    \n",
    "    print(f\"\\nImage Path: {authorized_image_path}\")\n",
    "    print(f\"Predicted Member: {auth_result['member']}\")\n",
    "    print(f\"Confidence: {auth_result['confidence']:.4f} ({auth_result['confidence']*100:.2f}%)\")\n",
    "    print(f\"Threshold: {CONFIDENCE_THRESHOLD*100:.0f}%\")\n",
    "    print(f\"\\nDecision: {auth_result['reason']}\")\n",
    "    \n",
    "    if auth_result['authorized']:\n",
    "        print(\" ACCESS GRANTED - User can proceed to voice verification\")\n",
    "    else:\n",
    "        print(\" ACCESS DENIED - User cannot proceed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Test will run once authorized image is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af63c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST2: Test Unauthorized/Unknown Access\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: UNAUTHORIZED USER - Unknown/Suspicious Face\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNote: To test unauthorized access, use:\")\n",
    "print(\"  - An unknown person's face (not in training data)\")\n",
    "print(\"  - A heavily disguised version of a known member\")\n",
    "print(\"  - A low-quality or partially obscured face\")\n",
    "print(\"\\nExample (if you have unauthorized image):\")\n",
    "print(\"  unauthorized_image_path = '../data/raw/images/unauthorized/unknown.jpg'\")\n",
    "print(\"  result = authenticate_user(unauthorized_image_path, model, reverse_label_encoder)\")\n",
    "\n",
    "try:\n",
    "    # load an unauthorized image if it exists\n",
    "    \n",
    "    unauthorized_paths = [\n",
    "        os.path.join(raw_images_path, 'unauthorized', 'unknown6.jpg'),\n",
    "        os.path.join(raw_images_path, 'unknown1.jpg'),\n",
    "    ]\n",
    "    \n",
    "    unauthorized_image_path = None\n",
    "    for path in unauthorized_paths:\n",
    "        if os.path.exists(path):\n",
    "            unauthorized_image_path = path\n",
    "            break\n",
    "    \n",
    "    if unauthorized_image_path:\n",
    "        print(f\"\\nFound unauthorized image at: {unauthorized_image_path}\")\n",
    "        \n",
    "        unauth_result = authenticate_user(unauthorized_image_path, model, reverse_label_encoder)\n",
    "        \n",
    "        print(f\"\\nImage Path: {unauthorized_image_path}\")\n",
    "        print(f\"Closest Match: {unauth_result['member']}\")\n",
    "        print(f\"Confidence: {unauth_result['confidence']:.4f} ({unauth_result['confidence']*100:.2f}%)\")\n",
    "        print(f\"Threshold: {CONFIDENCE_THRESHOLD*100:.0f}%\")\n",
    "        print(f\"\\nDecision: {unauth_result['reason']}\")\n",
    "        \n",
    "        if unauth_result['authorized']:\n",
    "            print(\"\\n WARNING - Unknown user was authorized (confidence too high)\")\n",
    "        else:\n",
    "            print(\"\\n SECURITY OK - Unknown user was correctly rejected\")\n",
    "            print(\"   User cannot proceed with transaction\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"No unauthorized image found.\")\n",
    "        print(\"To test, create a folder and add an unknown person's image:\")\n",
    "        print(\"  1. Create: data/raw/images/unauthorized/\")\n",
    "        print(\"  2. Add image: unknown.jpg or similar\")\n",
    "        print(\"  3. Re-run this cell\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Unauthorized test will run when test image is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91901e9d",
   "metadata": {},
   "source": [
    "1. USER SUBMITS FACIAL IMAGE\n",
    "   ↓\n",
    "2. FACIAL RECOGNITION MODEL PROCESSES IMAGE\n",
    "   ↓\n",
    "3. MODEL EXTRACTS FEATURES & MAKES PREDICTION\n",
    "   ↓\n",
    "4. MODEL RETURNS PREDICTION + CONFIDENCE SCORE\n",
    "   ↓\n",
    "5. CHECK CONFIDENCE AGAINST THRESHOLD (70%)\n",
    "   │\n",
    "   ├─ If Confidence >= 70% → ✅ AUTHORIZED\n",
    "   │  └─ Proceed to Voice Verification Stage\n",
    "   │\n",
    "   └─ If Confidence < 70% → ❌ DENIED\n",
    "      └─ Access Denied - End Transaction\n",
    "   ↓\n",
    "6. DISPLAY RESULT TO USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135868cd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the complete facial recognition pipeline:\n",
    "\n",
    "### Key Steps:\n",
    "1. **Image Collection**: Loaded facial images from each team member\n",
    "2. **Augmentation**: Applied various augmentations to increase training data\n",
    "3. **Feature Extraction**: Extracted histogram, edge, and color moment features\n",
    "4. **Model Training**: Trained a Random Forest classifier\n",
    "5. **Evaluation**: Evaluated using Accuracy, F1-Score, and Confusion Matrix\n",
    "6. **Prediction**: Made predictions on test images\n",
    "\n",
    "### Output Files:\n",
    "- `image_features.csv`: All extracted features\n",
    "- `facial_recognition_model.pkl`: Trained model\n",
    "\n",
    "### Performance Metrics:\n",
    "- Accuracy\n",
    "- F1-Score\n",
    "- Confusion Matrix\n",
    "- Classification Report"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
